\section{GBDT模型}
\subsection{回归}
\subsection{分类}

输入

\begin{itemize}
    \item 样本集$\{(\vec{x}_i, y_i)\}_{i=1}^N$, 其中$\vec{x}_i$是一个$K$维向量, $y_i\in \{0,1\} $
    \item 关于$F(x)$可微损失函数$L$
          \begin{equation*}
              L(y_i, F(x_i)) = -\Big\{y_iF(x_i) - \log (1 + e^{F(x_i)})\Big\}
          \end{equation*}
\end{itemize}

算法

\begin{enumerate}
    \item 初始化常量函数 $F_0(x) = \argmin_\gamma \sum_{i=1}^NL(y_i, \gamma)$
    \item 对$m=1,2,\cdots, M$
          \begin{enumerate}
              \item 对每个样本，计算负梯度值(伪残差, Pseudo Residual)
                    \begin{equation*}
                        r_{i,m} = \Bigg[-\frac{\partial L(y_i, F(x_i))}{\partial F(x_i)}\Bigg]\Bigg|_{F(x)=F_{m-1}(x)} \quad \text{for} \quad i=1,2,\cdots,M
                    \end{equation*}
              \item 对$\{r_{i,m}\}_{i=1}^N$拟合一颗\textbf{回归决策树}, 并给出终端区域$R_{jm}, j=1,2,\cdots,J_m$
              \item 对$j=1,2,\cdots, J_m$ 计算
                    \begin{equation}\label{opt_gbdt_cls_reg_dt}
                        \gamma_{jm} = \argmin_{\gamma} \sum_{x_i\in R_{ij}}L(y_i, F_{m-1}(x_i) + \gamma)
                    \end{equation}
                    作为终端区域$R_{ij}$的预测
              \item 更新模型
                    \begin{equation*}
                        F_m(x) = F_{m-1}(x) + \nu \sum_{j=1}^{J_m}\gamma_{jm}I(x\in R_{jm})
                    \end{equation*}
          \end{enumerate}
\end{enumerate}

\begin{remark}
    在分类问题中, 所有决策树之和$F_m(x), m=0,1,\cdots, M$预测的目标是对数几率值, 即$\log(\text{odds ratio})$, 而不是直接的$y$。也正因为如此，$F_m(x)$给出的预测，不能直接与$y$做差来构造损失函数.而是要首先通过
    \begin{equation*}
        p = \frac{e^{\log(\text{odds ratio})}}{1 + e^{\log(\text{odds ratio})}}
    \end{equation*}
    这个变换给出概率的预测。然后再将$p$代负的似然函数中得到损失函数. 这与回归问题中, 直接预测$y$的大小是一个比较本质的区别.
\end{remark}

\begin{remark}
    损失函数$L$形态的由来. 对数似然函数定义为

    \begin{equation*}
        \sum_{i=1}^N\Big\{y_i\log p_i + (1 - y_i)\log (1 - p_i)\Big\}
    \end{equation*}

    模型的损失函数定义为对数似然函数的相反数, 最小化总损失函数即最大化似然函数, 即

    \begin{equation*}
        L^*(\{(\vec{x}_i, y_i)\}_{i=1}^N) = -\sum_{i=1}^N\Big\{y_i\log p_i + (1 - y_i)\log (1 - p_i)\Big\}
    \end{equation*}

    整理右边,记$u_i$是第$i$个样本的$\log(\text{odds ratio})$, 该样本对应的$y_i=1$的概率$P(y_i=1)$记为$p_i$, 结合$(u_i, p_i)$的定义可知

    \begin{equation*}
        u_i = \log\frac{p_i}{1 - p_i},\quad p_i = \frac{e^{u_i}}{1 + e^{u_i}}
    \end{equation*}

    得到

    \begin{equation*}
        L^*(\{(\vec{x}_i, y_i)\}_{i=1}^N) = L^*(\{(u_i, y_i)\}_{i=1}^N) = -\sum_{i=1}^N\Big\{y_iu_i - \log(1 + e^{u_i}) \Big\} = \sum_{i=1}^NL(y_i,u_i)
    \end{equation*}

    其中

    \begin{equation*}
        L(y_i, u_i) = -\Big\{y_iu_i - \log(1 + e^{u_i})\Big\}
    \end{equation*}
    
    此即损失函数形态的由来.

\end{remark}
    
\begin{remark}
    对$i=1,2,\cdots, N$, 求$L(y_i, u_i)$关于$u_i$的偏导数有

    \begin{equation}
        \frac{\partial L(y_i, u_i)}{\partial u_i} = -\Big\{y_i - \frac{e^{u_i}}{1 + e^{u_i}} \Big\} =  -(y_i - p_i)
    \end{equation}

    其二阶导为
    \begin{equation}
        \frac{\partial^2 L(y_i, u_i)}{\partial u_i^2} = \frac{\partial p_i}{\partial u_i} = \frac{e^{u_i}}{(1 + e^{u_i})^2} = p_i(1-p_i)
    \end{equation}

    若给定$F(x)$, 记$u_i=F(x_i)$, 那么上面两式可以写为

    \begin{equation}\label{partial_12}
        \left\{
        \begin{array}{rcl}
            \frac{\partial l(y_i, F(x_i))}{\partial F(x_i)}     & = & -(y_i - p_i) \\
            \frac{\partial^2 l(y_i, F(x_i))}{\partial F(x_i)^2} & = & p_i(1-p_i)
        \end{array}
        \right.
    \end{equation}

    其中
    \begin{equation*}
        p_i = \frac{e^{F(x_i)}}{1 + e^{F(x_i)}}
    \end{equation*}

\end{remark}

\begin{remark}
    对问题\ref{opt_gbdt_cls_reg_dt}的直接求解很复杂, 因此采用近似处理的方法, 将问题右边在$F_{m-1}(x_i)$处泰勒展开可以得到

    \begin{equation*}
        L_j^* =\sum_{x_i\in R_{ij}}L(y_i, F_{m-1}(x_i) + \gamma) \approx \sum_{x_i\in R_{ij}}\Big\{L(y_i, F_{m-1}(x_i)) + \frac{\partial L(y_i, F_{m-1}(x_i))}{\partial F_{m-1}(x_i)}\gamma + \frac{1}{2}\frac{\partial^2 L}{\partial F_{m-1}(x_i)^2}\gamma^2\Big\}
    \end{equation*}

    对两边求$\gamma$的一阶导函数有
    \begin{equation*}
        \frac{\partial L_j^*}{\partial \gamma} = \sum_{x_i\in R_{ij}} \Big\{\frac{\partial L(y_i, F_{m-1}(x_i))}{\partial F_{m-1}(x_i)} + \frac{\partial^2 L(y_i, F_{m-1}(x_i))}{\partial F_{m-1}(x_i)^2}\gamma\Big\}
    \end{equation*}

    令其为零, 并参考\ref{partial_12}, 取$u_i=F_{m-1}(x_i), p_{i, m-1} = \frac{e^{F_{m-1}(x_i)}}{1 + e^{F_{m-1}(x_i)} }$, 有
    \begin{equation}
        \gamma = \frac{-\sum_{x_i\in R_{ij}}\frac{\partial L(y_i, F_{m-1}(x_i))}{\partial F_{m-1}(x_i)}}{\sum_{x_i\in R_{ij}}\frac{\partial^2 L(y_i, F_{m-1}(x_i))}{\partial F_{m-1}(x_i)^2}} = \frac{\sum_{x_i\in R_{ij}}(y_i-p_{i, m-1})}{\sum_{x_i\in R_{ij}}p_{i, m-1}(1-p_{i, m-1})}
    \end{equation}

\end{remark}

